{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "702252dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "src_path = r'C:\\Users\\rshaw\\Desktop\\EC Utbildning - Data Science\\Kurs 9 - Project\\Project\\ds23_projektkurs\\predictive-maintenance\\src\\Data processing scripts'\n",
    "sys.path.append(src_path)\n",
    "\n",
    "from Logging_module import LoggerSetup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b66ec8b",
   "metadata": {},
   "source": [
    "### Import Libraries & Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d64a3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sqlalchemy import create_engine\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72587a2f",
   "metadata": {},
   "source": [
    "### Load pre-processed data from SQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "267abcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_logger = LoggerSetup(logger_name='Data_Logger', log_file='data_log.log').get_logger()\n",
    "engine = create_engine('mssql+pyodbc://MSI/predictive_maintenance_db?driver=ODBC+Driver+17+for+SQL+Server&trusted_connection=yes')\n",
    "try:\n",
    "    data_logger.info(\"Loading 'PdM_errors' data from the database.\")\n",
    "    errors_df = pd.read_sql('SELECT * FROM PdM_errors', con=engine)\n",
    "    data_logger.info(\"'PdM_errors' data loaded successfully.\")\n",
    "\n",
    "    data_logger.info(\"Loading 'PdM_failures' data from the database.\")\n",
    "    failures_df = pd.read_sql('SELECT * FROM PdM_failures', con=engine)\n",
    "    data_logger.info(\"'PdM_failures' data loaded successfully.\")\n",
    "\n",
    "    data_logger.info(\"Loading 'PdM_machines' data from the database.\")\n",
    "    machines_df = pd.read_sql('SELECT * FROM PdM_machines', con=engine)\n",
    "    data_logger.info(\"'PdM_machines' data loaded successfully.\")\n",
    "\n",
    "    data_logger.info(\"Loading 'PdM_maint' data from the database.\")\n",
    "    maint_df = pd.read_sql('SELECT * FROM PdM_maint', con=engine)\n",
    "    data_logger.info(\"'PdM_maint' data loaded successfully.\")\n",
    "\n",
    "    data_logger.info(\"Loading 'PdM_telemetry' data from the database.\")\n",
    "    telemetry_df = pd.read_sql('SELECT * FROM PdM_telemetry', con=engine)\n",
    "    data_logger.info(\"'PdM_telemetry' data loaded successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    data_logger.error(f\"Error occurred while loading data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3000ac",
   "metadata": {},
   "source": [
    "### Read datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783002be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Errors DataFrame:\")\n",
    "display(errors_df.head())\n",
    "\n",
    "print(\"Failures DataFrame:\")\n",
    "display(failures_df.head())\n",
    "\n",
    "print(\"Machines DataFrame:\")\n",
    "display(machines_df.head())\n",
    "\n",
    "print(\"Maintenance DataFrame:\")\n",
    "display(maint_df.head())\n",
    "\n",
    "print(\"Telemetry DataFrame:\")\n",
    "display(telemetry_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1e908d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(telemetry_df.head())\n",
    "print(telemetry_df['datetime'].min(), telemetry_df['datetime'].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8026a82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "telemetry_df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafd897e",
   "metadata": {},
   "source": [
    "- Here we see sensor data for 4 operational settings that occurred over 12 months between January 1, 2015, and January 01, 2016. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed16be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(failures_df.head())\n",
    "print(failures_df['datetime'].min(), failures_df['datetime'].max())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7088c5bb",
   "metadata": {},
   "source": [
    "- Here we see that failures occurred over 12 months between January 2, 2015, and December 31, 2015. - \n",
    "- Failures involve different components (comp1, comp2, comp4), all related to machine ID 1 during this time range. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad86085",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12f06fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we drill down to focus on one machine from 100, namely machine no. 17\n",
    "df_sel_17 = telemetry_df.loc[telemetry_df['machineID'] == 17].reset_index(drop=True)\n",
    "print(df_sel_17.head(n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6639f0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sel_17.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60a21eb",
   "metadata": {},
   "source": [
    " - This machine number 17 has 8761 rows and 7 columns, including telemetry data (volt, rotate, pressure, vibration) and machineID.\n",
    "- All columns are numeric except for the datetime column which is a timestamp, and date which is an object type (string).\n",
    "- Memory used is approx 479.2 KB \n",
    "- There are no missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5bb339",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_sel_17[['volt', 'rotate', 'pressure', 'vibration']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287f16e4",
   "metadata": {},
   "source": [
    "- Sensor Ranges: All sensor readings (voltage, rotation, pressure, vibration) show reasonable variability, with voltage and rotation having the highest range of values.\n",
    "\n",
    "- Relatively Low Variation in Vibration: The vibration sensor has the lowest standard deviation, meaning its values are more tightly clustered around the mean compared to other sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390b8851",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sel_17.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b951b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(df_sel_17.isnull().sum() / df_sel_17.isnull().count() * 100, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6ea6a3",
   "metadata": {},
   "source": [
    "- This is just an additional check, but we can confirm here that there are no missing values and that our pre-processing of the raw datasets cleared any duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53c9dfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram_boxplot(df_sel_17, feature, figsize=(12, 7), kde=False, bins=None):\n",
    "    \"\"\"\n",
    "    Boxplot and histogram combined\n",
    "\n",
    "    data: dataframe\n",
    "    feature: dataframe column\n",
    "    figsize: size of figure (default (12,7))\n",
    "    kde: whether to the show density curve (default False)\n",
    "    bins: number of bins for histogram (default None)\n",
    "    \"\"\"\n",
    "    f2, (ax_box2, ax_hist2) = plt.subplots(\n",
    "        nrows=2,  \n",
    "        sharex=True,  \n",
    "        gridspec_kw={\"height_ratios\": (0.25, 0.75)},\n",
    "        figsize=figsize,\n",
    "    )  \n",
    "    sns.boxplot(\n",
    "        data=df_sel_17, x=feature, ax=ax_box2, showmeans=True, color=\"violet\"\n",
    "    )  \n",
    "    sns.histplot(\n",
    "        data=df_sel_17, x=feature, kde=kde, ax=ax_hist2, bins=bins, palette=\"winter\"\n",
    "    ) if bins else sns.histplot(\n",
    "        data=df_sel_17, x=feature, kde=kde, ax=ax_hist2\n",
    "    )  \n",
    "    ax_hist2.axvline(\n",
    "        df_sel_17[feature].mean(), color=\"green\", linestyle=\"--\"\n",
    "    ) \n",
    "    ax_hist2.axvline(\n",
    "        df_sel_17[feature].median(), color=\"black\", linestyle=\"-\"\n",
    "    )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cdeba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_numeric(series):\n",
    "    return pd.api.types.is_numeric_dtype(series)\n",
    "\n",
    "for feature in df_sel_17.columns:\n",
    "    if is_numeric(df_sel_17[feature]) and feature != 'machineID':\n",
    "        histogram_boxplot(df_sel_17, feature, figsize=(12, 7), kde=False, bins=None)\n",
    "    else:\n",
    "        print(f\"Skipping column: {feature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215faab4",
   "metadata": {},
   "source": [
    "- Most of the sensors show a fairly normal distribution, with the mean and median values aligning closely.\n",
    "\n",
    "- Standard Deviation appears to vary across the sensors, with a considerable portion of sensor readings showing spread beyond typical ranges. While some distributions have a higher concentration around the mean, sensors like pressure and voltage show a greater spread with extreme outliers.\n",
    "\n",
    "- Outliers are present across all four sensors. The voltage and rotation sensors have several outliers on both the lower and upper ends. While these outliers exist, they don't seem to significantly distort the central values (mean and median). The pressure and vibration sensors also show a reasonable number of outliers on the higher end.\n",
    "\n",
    "- The rotation sensor shows a slight right skew, as shown by a few high-value outliers stretching beyond the bulk of the data. Meanwhile, vibration shows a slight left skew, with more values on the lower side, though it is less pronounced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377df87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.lineplot(x='datetime', y='volt', data=df_sel_17)\n",
    "plt.title('Voltage Over Time')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.lineplot(x='datetime', y='rotate', data=df_sel_17)\n",
    "plt.title('Rotation Over Time')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.lineplot(x='datetime', y='pressure', data=df_sel_17)\n",
    "plt.title('Pressure Over Time')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.lineplot(x='datetime', y='vibration', data=df_sel_17)\n",
    "plt.title('Vibration Over Time')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dad2e25",
   "metadata": {},
   "source": [
    "- There seems to be quite a few spikes that could be anomalies leading to errors or failures.\n",
    "- We'll investigate further using z-scores to see how many standard deviations a sensor data point is from the mean, where we will target any point outside of -3 and +3 as anomaly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c5a6080",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Create plots of Z-scores for this machine to check for anomalies over time in sensor data \n",
    "scaler = StandardScaler()\n",
    "z_scores = scaler.fit_transform(df_sel_17[['volt', 'rotate', 'pressure', 'vibration']])\n",
    "z_scores_df = pd.DataFrame(z_scores, columns=['volt_zscore', 'rotate_zscore', 'pressure_zscore', 'vibration_zscore'])\n",
    "df_anomalies = pd.concat([df_sel_17, z_scores_df], axis=1)\n",
    "anomalies = df_anomalies[\n",
    "    (df_anomalies['volt_zscore'].abs() > 3) |\n",
    "    (df_anomalies['rotate_zscore'].abs() > 3) |\n",
    "    (df_anomalies['pressure_zscore'].abs() > 3) |\n",
    "    (df_anomalies['vibration_zscore'].abs() > 3)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f879ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anomalies['datetime'] = pd.to_datetime(df_anomalies['datetime'])\n",
    "fig, axs = plt.subplots(4, 1, figsize=(12, 16), sharex=True)\n",
    "\n",
    "axs[0].plot(df_anomalies['datetime'], df_anomalies['volt_zscore'], label='Volt Z-score', color='b')\n",
    "axs[0].set_ylabel('Volt Z-score')\n",
    "axs[0].axhline(y=3, color='r', linestyle='--')\n",
    "axs[0].axhline(y=-3, color='r', linestyle='--')\n",
    "axs[0].set_title('Volt Z-scores Over Time')\n",
    "\n",
    "axs[1].plot(df_anomalies['datetime'], df_anomalies['rotate_zscore'], label='Rotate Z-score', color='g')\n",
    "axs[1].set_ylabel('Rotate Z-score')\n",
    "axs[1].axhline(y=3, color='r', linestyle='--')\n",
    "axs[1].axhline(y=-3, color='r', linestyle='--')\n",
    "axs[1].set_title('Rotate Z-scores Over Time')\n",
    "\n",
    "axs[2].plot(df_anomalies['datetime'], df_anomalies['pressure_zscore'], label='Pressure Z-score', color='orange')\n",
    "axs[2].set_ylabel('Pressure Z-score')\n",
    "axs[2].axhline(y=3, color='r', linestyle='--')\n",
    "axs[2].axhline(y=-3, color='r', linestyle='--')\n",
    "axs[2].set_title('Pressure Z-scores Over Time')\n",
    "\n",
    "axs[3].plot(df_anomalies['datetime'], df_anomalies['vibration_zscore'], label='Vibration Z-score', color='purple')\n",
    "axs[3].set_ylabel('Vibration Z-score')\n",
    "axs[3].axhline(y=3, color='r', linestyle='--')\n",
    "axs[3].axhline(y=-3, color='r', linestyle='--')\n",
    "axs[3].set_title('Vibration Z-scores Over Time')\n",
    "\n",
    "axs[3].set_xlabel('Datetime')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "correlation_matrix = df_anomalies[['volt_zscore', 'rotate_zscore', 'pressure_zscore', 'vibration_zscore']].corr()\n",
    "plt.figure(figsize=(8, 6)) \n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, linewidths=0.5)\n",
    "plt.title('Correlation Matrix of Z-Scores for Sensors')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec813a8",
   "metadata": {},
   "source": [
    "- Both pressure and vibration seem to have more frequent spikes exceeding the threshold, indicating more frequent anomalies.\n",
    "\n",
    "- The most notable correlation is between the rotate_zscore and vibration_zscore (0.63), meaning these sensors tend to show anomalies at the same time. On the other hand, volt_zscore and pressure_zscore have a moderate negative correlation (-0.60), indicating an inverse relationship.\n",
    "\n",
    "- These anomalies and correlations give us a good reason to zoom into the time period with most visual peaks during during the first quarter of 2015, to investigate potential patterns, causes, and behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c516858f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anomalies['datetime'] = pd.to_datetime(df_anomalies['datetime'])\n",
    "start_date = '2015-01-01'\n",
    "end_date = '2015-03-31'\n",
    "df_time_window = df_anomalies[(df_anomalies['datetime'] >= start_date) & (df_anomalies['datetime'] <= end_date)]\n",
    "fig, axs = plt.subplots(4, 1, figsize=(12, 16), sharex=True)\n",
    "\n",
    "axs[0].plot(df_time_window['datetime'], df_time_window['volt_zscore'], label='Volt Z-score', color='b')\n",
    "axs[0].set_ylabel('Volt Z-score')\n",
    "axs[0].axhline(y=3, color='r', linestyle='--')\n",
    "axs[0].axhline(y=-3, color='r', linestyle='--')\n",
    "axs[0].set_title('Volt Z-scores Over Time (Filtered Time Window)')\n",
    "\n",
    "axs[1].plot(df_time_window['datetime'], df_time_window['rotate_zscore'], label='Rotate Z-score', color='g')\n",
    "axs[1].set_ylabel('Rotate Z-score')\n",
    "axs[1].axhline(y=3, color='r', linestyle='--')\n",
    "axs[1].axhline(y=-3, color='r', linestyle='--')\n",
    "axs[1].set_title('Rotate Z-scores Over Time (Filtered Time Window)')\n",
    "\n",
    "axs[2].plot(df_time_window['datetime'], df_time_window['pressure_zscore'], label='Pressure Z-score', color='orange')\n",
    "axs[2].set_ylabel('Pressure Z-score')\n",
    "axs[2].axhline(y=3, color='r', linestyle='--')\n",
    "axs[2].axhline(y=-3, color='r', linestyle='--')\n",
    "axs[2].set_title('Pressure Z-scores Over Time (Filtered Time Window)')\n",
    "\n",
    "axs[3].plot(df_time_window['datetime'], df_time_window['vibration_zscore'], label='Vibration Z-score', color='purple')\n",
    "axs[3].set_ylabel('Vibration Z-score')\n",
    "axs[3].axhline(y=3, color='r', linestyle='--')\n",
    "axs[3].axhline(y=-3, color='r', linestyle='--')\n",
    "axs[3].set_title('Vibration Z-scores Over Time (Filtered Time Window)')\n",
    "\n",
    "axs[3].set_xlabel('Datetime')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502f49c2",
   "metadata": {},
   "source": [
    "Overall, this first-quarter analysis highlights significant fluctuations, especially in voltage, pressure, and vibration, with several data points indicating potential anomalies or outliers. These might correlate with system irregularities or potential failures if left unchecked."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88cb135",
   "metadata": {},
   "source": [
    "### Data Preprocessing (in addition to Missis scripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2938dca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we convert model to an integer to limit features for the model later to avoid multi-collinearity and overfitting.\n",
    "model_mapping = {value: i for i, value in enumerate(machines_df['model'].unique(), start=1)}\n",
    "machines_df['model'] = machines_df['model'].map(model_mapping)\n",
    "\n",
    "data_logger.info(f\"Model mapping completed successfully. Mapping: {model_mapping}\")\n",
    "display(machines_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5e19dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deal w categorical variables\n",
    "failures_df = pd.get_dummies(failures_df, columns=['failure'], prefix='fail', dtype=int)\n",
    "maint_df = pd.get_dummies(maint_df, columns=['comp'], prefix='maint', dtype=int)\n",
    "errors_df = pd.get_dummies(errors_df, columns=['errorID'], prefix='', prefix_sep='', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a323e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_df.rename(columns={'datetime': 'datetime_error'}, inplace=True)\n",
    "failures_df.rename(columns={'datetime': 'datetime_failure'}, inplace=True)\n",
    "maint_df.rename(columns={'datetime': 'datetime_maint'}, inplace=True)\n",
    "telemetry_df.rename(columns={'datetime': 'datetime_telemetry'}, inplace=True)\n",
    "\n",
    "errors_df['datetime_error'] = pd.to_datetime(errors_df['datetime_error'])\n",
    "failures_df['datetime_failure'] = pd.to_datetime(failures_df['datetime_failure'])\n",
    "maint_df['datetime_maint'] = pd.to_datetime(maint_df['datetime_maint'])\n",
    "telemetry_df['datetime_telemetry'] = pd.to_datetime(telemetry_df['datetime_telemetry'])\n",
    "\n",
    "print(errors_df['datetime_error'].dtypes)\n",
    "print(failures_df['datetime_failure'].dtypes)\n",
    "print(maint_df['datetime_maint'].dtypes)\n",
    "print(telemetry_df['datetime_telemetry'].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208b6a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create failure, error and maintenance flags to prep for feature engineering\n",
    "failures_df['failure_flag'] = 1\n",
    "failure_count = failures_df['failure_flag'].sum()\n",
    "\n",
    "errors_df['error_flag'] = 1\n",
    "error_count = errors_df['error_flag'].sum()\n",
    "\n",
    "maint_df['maint_flag'] = 1\n",
    "maintenance_count = maint_df['maint_flag'].sum()\n",
    "\n",
    "print(f\"Number of failures: {failure_count}\")\n",
    "print(f\"Number of errors: {error_count}\")\n",
    "print(f\"Number of maintenances: {maintenance_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce67fb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "maint_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b6861c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(telemetry_df.shape)\n",
    "print(errors_df.shape)\n",
    "print(failures_df.shape)\n",
    "print(maint_df.shape)\n",
    "print(machines_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd84fc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(telemetry_df.head())\n",
    "display(errors_df.head())\n",
    "display(failures_df.head())\n",
    "display(maint_df.head())\n",
    "display(machines_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2ec5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Merge Data before feature engineering\n",
    "telemetry_df = telemetry_df.sort_values('datetime_telemetry')\n",
    "failures_df = failures_df.sort_values('datetime_failure')\n",
    "errors_df = errors_df.sort_values('datetime_error')\n",
    "maintenance_df = maint_df.sort_values('datetime_maint')\n",
    "\n",
    "merged_df = pd.merge_asof(\n",
    "    telemetry_df,\n",
    "    failures_df[['machineID', 'datetime_failure', 'fail_comp1', 'fail_comp2', 'fail_comp3', 'fail_comp4', 'failure_flag']],\n",
    "    by='machineID',\n",
    "    left_on='datetime_telemetry',\n",
    "    right_on='datetime_failure',\n",
    "    direction='backward',\n",
    "    tolerance=pd.Timedelta('1d')\n",
    ")\n",
    "\n",
    "merged_df = pd.merge_asof(\n",
    "    merged_df,\n",
    "    errors_df[['machineID', 'datetime_error', 'error1', 'error2', 'error3', 'error4', 'error5', 'error_flag']],\n",
    "    by='machineID',\n",
    "    left_on='datetime_telemetry',\n",
    "    right_on='datetime_error',\n",
    "    direction='backward',\n",
    "    tolerance=pd.Timedelta('1d')\n",
    ")\n",
    "\n",
    "merged_df = pd.merge_asof(\n",
    "    merged_df,\n",
    "    maintenance_df[['machineID', 'datetime_maint', 'maint_comp1', 'maint_comp2', 'maint_comp3', 'maint_comp4', 'maint_flag']],\n",
    "    by='machineID',\n",
    "    left_on='datetime_telemetry',\n",
    "    right_on='datetime_maint',\n",
    "    direction='backward',\n",
    "    tolerance=pd.Timedelta('7d')\n",
    ")\n",
    "\n",
    "merged_df = pd.merge(\n",
    "    merged_df,\n",
    "    machines_df[['machineID', 'model', 'age']],\n",
    "    on='machineID',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "merged_df.fillna(0, inplace=True)\n",
    "\n",
    "indicator_columns = ['failure_flag', 'error_flag', 'maint_flag', 'fail_comp1', 'fail_comp2', 'fail_comp3', 'fail_comp4', 'error1', 'error2', 'error3', 'error4', 'error5', 'maint_comp1', 'maint_comp2', 'maint_comp3', 'maint_comp4']\n",
    "\n",
    "for col in indicator_columns:\n",
    "    if col in merged_df.columns:\n",
    "        merged_df[col] = merged_df[col].astype(int)\n",
    "\n",
    "display(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e20d090",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec083d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_id_to_plot = 17\n",
    "plot_df = merged_df[merged_df['machineID'] == machine_id_to_plot]\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.plot(plot_df['datetime_telemetry'], plot_df['pressure'], label='Pressure', color='blue', linewidth=2)\n",
    "plt.plot(plot_df['datetime_telemetry'], plot_df['vibration'], label='Vibration', color='green', linewidth=2)\n",
    "if 'failure_flag' in plot_df.columns:\n",
    "    plt.step(plot_df['datetime_telemetry'], plot_df['failure_flag'] * 50, label='Failure Flag', color='purple', linestyle='--', where='post')\n",
    "if 'error_flag' in plot_df.columns:\n",
    "    plt.step(plot_df['datetime_telemetry'], plot_df['error_flag'] * 50, label='Error Flag', color='orange', linestyle='--', where='post')\n",
    "if 'maint_flag' in plot_df.columns:\n",
    "    plt.step(plot_df['datetime_telemetry'], plot_df['maint_flag'] * 50, label='Maintenance Flag', color='red', linestyle='--', where='post')\n",
    "plt.xlabel('Datetime')\n",
    "plt.ylabel('Telemetry Readings and Event Flags')\n",
    "plt.title(f'Telemetry Readings and Event Indicators for Machine {machine_id_to_plot}')\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529c8830",
   "metadata": {},
   "source": [
    "- While for only 1 machine, this still shows a trend of reactive mainteance until the last quarter of 2015 and then a major uturn in proactive maintenace at the end of the year. \n",
    "- Errors are consistent and can lead to failures but the correlation isn't clear. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59224bff",
   "metadata": {},
   "source": [
    "### Feature Engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8f89be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # RUL for machines\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Handling Missing Timestamps and Calculating 'RUL'\n",
    "merged_df['datetime_failure'] = merged_df['datetime_telemetry'].where(merged_df['failure_flag'] == 1)\n",
    "merged_df['datetime_failure'] = merged_df.groupby('machineID')['datetime_failure'].bfill()\n",
    "merged_df['datetime_telemetry'] = pd.to_datetime(merged_df['datetime_telemetry'])\n",
    "merged_df['datetime_failure'] = pd.to_datetime(merged_df['datetime_failure'], errors='coerce')\n",
    "\n",
    "num_na_rul = merged_df['datetime_failure'].isna().sum()\n",
    "print(f\"Number of rows with missing failure values (NaT): {num_na_rul}\")\n",
    "\n",
    "merged_df['RUL'] = (merged_df['datetime_failure'] - merged_df['datetime_telemetry']).dt.total_seconds() / 3600\n",
    "\n",
    "mean_rul = merged_df['RUL'].mean()\n",
    "print(f\"Mean RUL used to impute missing values: {mean_rul}\")\n",
    "\n",
    "merged_df['RUL'].fillna(mean_rul, inplace=True)\n",
    "\n",
    "rul_data = merged_df['RUL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "35fd7436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time since last maintenance for machines (in hours)\n",
    "merged_df['time_since_last_maint'] = (\n",
    "    merged_df.groupby('machineID')['datetime_telemetry']\n",
    "    .transform(lambda x: x - x.where(merged_df['maint_flag'] == 1).ffill())\n",
    ").dt.total_seconds() / 3600\n",
    "merged_df['time_since_last_maint'] = merged_df['time_since_last_maint'].fillna(99999)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7da7a8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Time since last failure for components\n",
    "# # components = ['fail_comp1', 'fail_comp2', 'fail_comp3', 'fail_comp4']\n",
    "# for component in components:\n",
    "#     merged_df[f'time_since_last_failure_{component}'] = (\n",
    "#         merged_df.groupby('machineID')['datetime_telemetry']\n",
    "#         .transform(lambda x: x - x.where(merged_df[component] == 1).ffill())\n",
    "#     ).dt.total_seconds() / 3600\n",
    "#     merged_df[f'time_since_last_failure_{component}'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69680f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 Check RUL values after failure timestamps (should be 0)\n",
    "df_after_failure = merged_df[merged_df['failure_flag'] == 1]\n",
    "\n",
    "# Check if any RUL is greater than 0 after failure\n",
    "incorrect_rul_after_failure = df_after_failure[df_after_failure['RUL'] > 0]\n",
    "\n",
    "if not incorrect_rul_after_failure.empty:\n",
    "    print(\"RUL values after failure that are not 0:\")\n",
    "    display(incorrect_rul_after_failure[['machineID', 'datetime_telemetry', 'failure_flag', 'RUL']])\n",
    "else:\n",
    "    print(\"All RUL values after failure are 0.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c65006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where failure has not occurred yet (failure_flag == 0)\n",
    "df_before_failure = merged_df[merged_df['failure_flag'] == 0]\n",
    "df_before_failure = df_before_failure.sort_values(by=['machineID', 'datetime_telemetry'])\n",
    "\n",
    "# Calculate the RUL difference for each machine (to check if RUL is decreasing)\n",
    "df_before_failure['RUL_diff'] = df_before_failure.groupby('machineID')['RUL'].diff()\n",
    "incorrect_rul_before_failure = df_before_failure[df_before_failure['RUL_diff'] < 0]\n",
    "\n",
    "# Show rows where RUL is increasing (which shouldn't happen)\n",
    "if incorrect_rul_before_failure.empty:\n",
    "    print(\"RUL values before failure that are not decreasing:\")\n",
    "    display(incorrect_rul_before_failure[['machineID', 'datetime_telemetry', 'RUL', 'RUL_diff']])\n",
    "else:\n",
    "    print(\"All RUL values before failure are decreasing correctly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e23659",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 Check how many rows do not have a scheduled failure (next failure date is in the far future)\n",
    "merged_df['datetime_failure'] = pd.to_datetime(merged_df['datetime_failure'], errors='coerce') \n",
    "merged_df['datetime_next_failure'] = merged_df.groupby('machineID')['datetime_failure'].bfill()\n",
    "\n",
    "if 'datetime_next_failure' in merged_df.columns:\n",
    "    print(\"datetime_next_failure column successfully created.\")\n",
    "else:\n",
    "    print(\"datetime_next_failure column is missing.\")\n",
    "\n",
    "future_date = pd.Timestamp('2100-02-02')\n",
    "\n",
    "no_scheduled_failure = merged_df[merged_df['datetime_next_failure'] == future_date]\n",
    "\n",
    "num_no_scheduled_failure = len(no_scheduled_failure)\n",
    "print(f\"Number of rows with no scheduled failure: {num_no_scheduled_failure}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a5c5f9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Time since last maintenance for components\n",
    "# for comp in ['maint_comp1', 'maint_comp2', 'maint_comp3', 'maint_comp4']:\n",
    "#     merged_df[f'time_since_last_{comp}'] = (\n",
    "#         merged_df.groupby('machineID')['datetime_telemetry']\n",
    "#         # .transform(lambda x: x - x.where(merged_df[comp] == 1).ffill())\n",
    "#     ).dt.total_seconds() / 3600  # Convert to hours\n",
    "#     merged_df[f'time_since_last_{comp}'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05abdc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall maintenance history\n",
    "overall_maint_hist = merged_df['overall_maintenance_history'] = (\n",
    "    merged_df.groupby('machineID')['maint_flag']\n",
    "    .transform('sum')\n",
    ")\n",
    "overall_maint_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d48dfcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interaction features\n",
    "merged_df['pressure_x_volt'] = merged_df['pressure'] * merged_df['volt']\n",
    "merged_df['pressure_x_vibration'] = merged_df['pressure'] * merged_df['vibration']\n",
    "merged_df['rotate_x_vibration'] = merged_df['rotate'] * merged_df['vibration']\n",
    "merged_df['rotate_x_volt'] = merged_df['rotate'] * merged_df['volt']\n",
    "\n",
    "# Lag features\n",
    "merged_df['vibration_t-1'] = merged_df['vibration'].shift(1)\n",
    "merged_df['pressure_t-1'] = merged_df['pressure'].shift(1)\n",
    "merged_df['volt_t-1'] = merged_df['volt'].shift(1)\n",
    "merged_df['rotate_t-1'] = merged_df['rotate'].shift(1)\n",
    "\n",
    "merged_df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74211abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "merged_df_numeric = merged_df.select_dtypes(include=['float64', 'int64', 'int32']).drop(columns=['RUL'])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(merged_df_numeric)\n",
    "\n",
    "isolation_forest = IsolationForest(n_estimators=100, contamination='auto', random_state=42)\n",
    "isolation_forest.fit(X_scaled)\n",
    "\n",
    "anomaly_labels = isolation_forest.predict(X_scaled)\n",
    "\n",
    "merged_df['anomaly_flag'] = (anomaly_labels == -1).astype(int)\n",
    "\n",
    "print(merged_df[['anomaly_flag']].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dd4da802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interaction features between anomalies, failures, and maintenance\n",
    "merged_df['anomaly_x_failure'] = merged_df['anomaly_flag'] * merged_df['failure_flag']\n",
    "merged_df['anomaly_x_maint'] = merged_df['anomaly_flag'] * merged_df['maint_flag']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2cd9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(merged_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64192bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all features are numeric before model build\n",
    "merged_df_numeric = merged_df.select_dtypes(include=['float64', 'int64', 'int32'])\n",
    "corr_matrix = merged_df_numeric.corr()\n",
    "merged_df_numeric.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc39e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot numeric features \n",
    "subset_columns = ['volt', 'rotate', 'pressure', 'vibration', 'pressure_x_volt', 'rotate_x_vibration', 'overall_maintenance_history', 'age']\n",
    "corr_subset = merged_df_numeric[subset_columns].corr()\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(corr_subset, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation between Telemetry, Interaction Terms, and Maintenance Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c603c7",
   "metadata": {},
   "source": [
    "- Interaction terms like pressure_x_volt and rotate_x_vibration are strongly correlated with their base features (e.g., pressure, volt, vibration), which may indicate redundancy unless they capture non-linear relationships.\n",
    "\n",
    "- Overall maintenance history and machine age show very weak correlations with sensor data, suggesting these features might not heavily influence real-time telemetry.\n",
    "\n",
    "- Therefore, we will apply Isolation Forest and Random Forest to help narrow down the feature selection process and identify the most relevant variables for model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fd187e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the distribution of target feature RUL\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(rul_data, bins=20, kde=True, color='blue')\n",
    "plt.title('Distribution of Remaining Useful Life (RUL)', fontsize=16)\n",
    "plt.xlabel('Remaining Useful Life (RUL)', fontsize=14)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d2e08f",
   "metadata": {},
   "source": [
    "- Presence of Imputed RUL Values: The sharp peak around the mean RUL indicates that many machines or components have missing failure information, leading to a concentration of imputed values.\n",
    "This can skew model training because the imputed RUL values do not represent real failure patterns and could distort model predictions.\n",
    "\n",
    "- Right-Skewed Distribution: The right-skewed nature of the distribution suggests that most components/machines have a relatively low RUL, with fewer instances of very high RUL values. This is typical in real-world predictive maintenance data, as machines are more likely to be closer to failure than far away from it.\n",
    "\n",
    "TO DO - Imputation Flag: Add a binary feature that flags rows where the RUL has been imputed. This might help the model distinguish between real and imputed data during prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83281ab7",
   "metadata": {},
   "source": [
    "#### Feature Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee26a0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d429f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_numeric.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f139e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import pandas as pd\n",
    "\n",
    "target = 'RUL'\n",
    "features = merged_df_numeric.drop(columns=[target]).columns.tolist()\n",
    "\n",
    "train_data = merged_df[(merged_df['datetime_telemetry'] >= '2014-06-01') & \n",
    "                       (merged_df['datetime_telemetry'] <= '2015-08-31')]\n",
    "val_data = merged_df[(merged_df['datetime_telemetry'] >= '2015-09-01') & \n",
    "                     (merged_df['datetime_telemetry'] <= '2015-10-31')]\n",
    "test_data = merged_df[(merged_df['datetime_telemetry'] >= '2015-11-01') & \n",
    "                      (merged_df['datetime_telemetry'] <= '2015-12-31')]\n",
    "\n",
    "X_train, y_train = train_data[features], train_data[target]\n",
    "X_val, y_val = val_data[features], val_data[target]\n",
    "X_test, y_test = test_data[features], test_data[target]\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=40, n_jobs=-1, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_val_pred = rf.predict(X_val)\n",
    "val_mae = mean_absolute_error(y_val, y_val_pred)\n",
    "print(f\"Random Forest Validation MAE: {val_mae}\")\n",
    "\n",
    "y_test_pred = rf.predict(X_test)\n",
    "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "print(f\"Random Forest Test MAE: {test_mae}\")\n",
    "\n",
    "importances = rf.feature_importances_\n",
    "feature_importance_df = pd.DataFrame({'Feature': features, 'Importance': importances})\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "print(feature_importance_df)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'], color='skyblue')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importances from Random Forest')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7416725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cumulative importance\n",
    "feature_importance_df['Cumulative Importance'] = feature_importance_df['Importance'].cumsum()\n",
    "selected_features = feature_importance_df[feature_importance_df['Cumulative Importance'] <= 0.95]['Feature'].tolist()\n",
    "print(f\"Selected features contributing to 95% of total importance: {selected_features}\")\n",
    "\n",
    "print(len(selected_features))\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(feature_importance_df['Cumulative Importance'], marker='o', linestyle='--')\n",
    "plt.axhline(y=0.95, color='r', linestyle='-')\n",
    "plt.title('Cumulative Feature Importance')\n",
    "plt.xlabel('Number of Features')\n",
    "plt.ylabel('Cumulative Importance')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9306c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features contributing to 95% of total importance\n",
    "selected_features = feature_importance_df[feature_importance_df['Cumulative Importance'] <= 0.95]['Feature'].tolist()\n",
    "merged_df_final = merged_df[selected_features]\n",
    "display(merged_df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9d1ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = merged_df_final.corr()\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed707185",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(merged_df_final)\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(scaled_data)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_.cumsum(), marker='o', linestyle='--')\n",
    "plt.title('Cumulative Explained Variance by PCA Components')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.axhline(y=0.90, color='r', linestyle='-')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "explained_variance = pca.explained_variance_ratio_.cumsum()\n",
    "print(\"Cumulative Explained Variance:\", explained_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d76881b",
   "metadata": {},
   "source": [
    "- Just under 98% of variance explained by 10 components so we'll use this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4942a22b",
   "metadata": {},
   "source": [
    "#### Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9ebd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, mean_absolute_error\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential, Model  # type: ignore\n",
    "from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout, Input, Conv1D, Dense, Flatten # type: ignore\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau  # type: ignore\n",
    "from tcn import TCN\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.SettingWithCopyWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "train_data = merged_df[(merged_df['datetime_telemetry'] >= '2014-06-01') &\n",
    "                       (merged_df['datetime_telemetry'] <= '2015-08-31')]\n",
    "val_data = merged_df[(merged_df['datetime_telemetry'] >= '2015-09-01') &\n",
    "                     (merged_df['datetime_telemetry'] <= '2015-10-31')]\n",
    "test_data = merged_df[(merged_df['datetime_telemetry'] >= '2015-11-01') &\n",
    "                      (merged_df['datetime_telemetry'] <= '2015-12-31')]\n",
    "\n",
    "train_data.reset_index(drop=True, inplace=True)\n",
    "val_data.reset_index(drop=True, inplace=True)\n",
    "test_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "reduced_features = merged_df_final.columns.tolist()\n",
    "\n",
    "train_data_reduced = train_data[reduced_features]\n",
    "val_data_reduced = val_data[reduced_features]\n",
    "test_data_reduced = test_data[reduced_features]\n",
    "\n",
    "num_features = len(reduced_features)\n",
    "\n",
    "# X (Independent variables)\n",
    "train_features_scaled = scaler.transform(train_data[reduced_features])\n",
    "val_features_scaled = scaler.transform(val_data[reduced_features])\n",
    "test_features_scaled = scaler.transform(test_data[reduced_features])\n",
    "\n",
    "n_components = 12\n",
    "pca = PCA(n_components=n_components)\n",
    "\n",
    "train_features_pca = pca.fit_transform(train_features_scaled)\n",
    "val_features_pca = pca.transform(val_features_scaled)\n",
    "test_features_pca = pca.transform(test_features_scaled)\n",
    "\n",
    "# y (dependent variables)\n",
    "train_regression_targets = train_data[['RUL']].values\n",
    "val_regression_targets = val_data[['RUL']].values\n",
    "test_regression_targets = test_data[['RUL']].values\n",
    "\n",
    "print(train_features_pca.shape)\n",
    "print(val_features_pca.shape)\n",
    "print(test_features_pca.shape)\n",
    "print(train_regression_targets.shape)\n",
    "print(val_regression_targets.shape)\n",
    "print(test_regression_targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd678863",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_sequences(features, regression_targets, sequence_length):\n",
    "    \"\"\"\n",
    "    Create sequences of features and corresponding regression targets for time series prediction.\n",
    "\n",
    "    Parameters:\n",
    "    - features: numpy array of shape (num_samples, num_features), input features.\n",
    "    - regression_targets: numpy array of shape (num_samples, num_targets), target for regression (e.g., RUL).\n",
    "    - sequence_length: int, the length of the sequences to create.\n",
    "\n",
    "    Returns:\n",
    "    - X: numpy array of shape (num_sequences, sequence_length, num_features), input sequences.\n",
    "    - y: numpy array of shape (num_sequences, num_targets), regression targets for the last time step of each sequence.\n",
    "    \"\"\"\n",
    "    X = np.array([features[i:i + sequence_length] for i in range(len(features) - sequence_length)])\n",
    "    y = np.array([regression_targets[i + sequence_length] for i in range(len(features) - sequence_length)])\n",
    "    return X, y\n",
    "\n",
    "sequence_length = 72  \n",
    "\n",
    "X_train, y_train = create_sequences(train_features_pca, train_regression_targets, sequence_length)\n",
    "X_val, y_val = create_sequences(val_features_pca, val_regression_targets, sequence_length)\n",
    "X_test, y_test = create_sequences(test_features_pca, test_regression_targets, sequence_length)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_val shape:\", X_val.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"X_train shape:\", y_train.shape)\n",
    "print(\"X_val shape:\", y_val.shape)\n",
    "print(\"X_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaf45c2",
   "metadata": {},
   "source": [
    "#### Model Build: Model Architecture and Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a8c709",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tcn\n",
    "from tcn import TCN\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.initializers import HeNormal\n",
    "from tensorflow.keras.losses import Huber\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "input_shape = (sequence_length, 10) #from PCA reduction\n",
    "inputs = Input(shape=input_shape)\n",
    "\n",
    "x = TCN(nb_filters=128, kernel_size=3, dilations=[1, 2, 4, 8, 16],\n",
    "        nb_stacks=3, padding='causal', kernel_initializer=HeNormal())(inputs)\n",
    "x = Dropout(0.6)(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "output_rul = Dense(1, activation='linear', name='regression_output_rul')(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=[output_rul])\n",
    "\n",
    "model.compile(optimizer=Adam(),\n",
    "              loss=Huber(),\n",
    "              metrics=['mean_absolute_error'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ff76ab",
   "metadata": {},
   "source": [
    "#### Model Preraration & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a311fc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import tensorflow as tf\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "train_data['RUL_clipped'] = np.clip(train_data['RUL'], 0, 4000)\n",
    "\n",
    "sample_weights = compute_sample_weight('balanced', train_data['RUL_clipped'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.00005)\n",
    "\n",
    "y_train = train_data['RUL_clipped'].values\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    sample_weight=sample_weights,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479f927b",
   "metadata": {},
   "source": [
    "##### Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2819695",
   "metadata": {},
   "source": [
    "##### Model Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9565f7",
   "metadata": {},
   "source": [
    "##### Model Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c47d3f",
   "metadata": {},
   "source": [
    "#### Key Insights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "1bebcc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "model_path = r'C:\\Users\\rshaw\\Desktop\\EC Utbildning - Data Science\\Kurs 9 - Project\\Project\\ds23_projektkurs\\predictive-maintenance\\models\\Trained models'\n",
    "model_save_path = os.path.join(model_path, 'tcn_multitask_model_v1.keras')\n",
    "model.save(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5859908d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d6b483",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('mssql+pyodbc://MSI/predictive_maintenance_db?driver=ODBC+Driver+17+for+SQL+Server&trusted_connection=yes')\n",
    "table_name = 'robs_merged_PdM_data'\n",
    "merged_df.to_sql(name=table_name, con=engine, if_exists='replace', index=False)\n",
    "\n",
    "print(f\"Table '{table_name}' has been successfully saved to the database.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e31b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# table_name = 'robs_processed_PdM_data'\n",
    "\n",
    "# # Save DataFrame to the SQL database as a new table\n",
    "# merged_df_final.to_sql(table_name, con=engine, if_exists='replace', index=False)\n",
    "\n",
    "# print(f\"Data saved successfully to '{table_name}' in the database.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
